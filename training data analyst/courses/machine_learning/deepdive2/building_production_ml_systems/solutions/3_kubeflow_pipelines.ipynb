{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow pipelines\n",
    "\n",
    "**Learning Objectives:**\n",
    "  1. Learn how to deploy a Kubeflow cluster on GCP\n",
    "  1. Learn how to use the notebook server on Kubeflow\n",
    "  1. Learn how to create a experiment in Kubeflow\n",
    "  1. Learn how to package you code into a Kubeflow pipeline\n",
    "  1. Learn how to run a Kubeflow pipeline in a repeatable and traceable way\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will first setup a Kubeflow cluster on GCP, and then launch a Kubeflow Notebook Server from where we will run this notebook. This will allow us to pilote the Kubeflow cluster from the notebook. Then, we will create a Kubeflow experiment and a Kubflow pipeline from our taxifare machine learning code. At last, we will run the pipeline on the Kubeflow cluster, providing us with a reproducible and traceable way to execute machine learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a Kubeflow cluster on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy a [Kubeflow](https://www.kubeflow.org/) cluster\n",
    "in your GCP project, use the [Kubeflow cluster deployer](https://deploy.kubeflow.cloud/#/deploy).\n",
    "\n",
    "There is a [setup video](https://www.kubeflow.org/docs/started/cloud/getting-started-gke/~) that will\n",
    "take you over all the steps in details, and explains how to access to the Kubeflow Dashboard UI, once it is \n",
    "running. \n",
    "\n",
    "You'll need to create an OAuth client for authentication purposes: Follow the \n",
    "instructions [here](https://www.kubeflow.org/docs/gke/deploy/oauth-setup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a Jupyter notebook server on the Kubeflow cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Kubeflow cluster allows you not only to run Kubeflow pipelines, but it also allows you to launch a Jupyter notebook server from which you can pilote the Kubeflow cluster. In particular, you can create experiments, define and run pipelines from whithin notebooks running on that Jupiter notebook server. This is exactly what we are going to do in this notebook.\n",
    "\n",
    "First of all, click on the \"Notebook Sever\" tab in the Kubeflow Dashboard UI, and create a Notebook Server. Once it's ready connect to it.\n",
    "\n",
    "Since the goal is to run this notebook on that Kubeflow Notebook Server, firt create new notebook and clone the training-data-analysis repo by running the following command in a cell and then naviguating to this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git clone -b ml_on_gcp-kubeflow_pipelines --single-branch https://github.com/GoogleCloudPlatform/training-data-analyst.git\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, you should be running this notebook from the Notebook Server from the Kubeflow cluster you created on your GCP project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating a Kubeflow client to pilote the Kubeflow cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the experiments that are running on this cluster. Since you just launched it, you should see only a single \"Default\" experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a 'taxifare' experiment where we could look at all the various runs of our taxifare pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = client.create_experiment(name='taxifare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the experiment has been created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging you code into Kubeflow components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have packaged our taxifare ml pipeline into three components:\n",
    "* `./components/bq2gcs` that creates the training and evaluation data from BigQuery and exports it to GCS\n",
    "* `./components/trainjob` that launches the training container on AI-platform and exports the model\n",
    "* `./components/deploymodel` that deploys the trained model to AI-platform as a REST API\n",
    "\n",
    "Each of these components has been wrapped into a Docker container, in the same way we did with the taxifare training code in the previous lab.\n",
    "\n",
    "If you inspect the code in these folders, you'll notice that the `main.py` or `main.sh` files contain the code we previously executed in the notebooks (loading the data to GCS from BQ, or launching a training job to AI-platform, etc.). The last line in the `Dockerfile` tells you that these files are executed when the container is run. \n",
    "So we just packaged our ml code into light container images for reproducibility. \n",
    "\n",
    "We have made it simple for you to build the container images and push them to the Google Cloud image registry gcr.io in your project: just type `make` in the pipelines directory! However, you can't do that from a Kubeflow notebook because Docker is not installed there. So you'll have to do that from Cloud Shell.\n",
    "\n",
    "For that, open Cloud Shell, and clone this repo there. Then cd to the pipelines subfolder:\n",
    "\n",
    "```bash\n",
    "$ git clone -b ml_on_gcp-kubeflow_pipelines --single-branch https://github.com/GoogleCloudPlatform/training-data-analyst.git\n",
    "\n",
    "$ cd training-data-analyst/courses/machine_learning/production_ml_systems/pipelines/\n",
    "```\n",
    "\n",
    "Then run `make` to build and push the images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the container images are pushed to the regsitry in your project, we need to create yaml files describing to Kubeflow how to use these containers. It boils down essentially \n",
    "* describing what arguments Kubeflow needs to pass to the containers when it runs them\n",
    "* telling Kubeflow where to fetch the corresponding Docker images\n",
    "\n",
    "In the cells below, we have three of these \"Kubeflow component description files\", one for each of our components.\n",
    "\n",
    "For each of these, correct the image URI to reflect that you pushed the images into the gcr.io associated with your project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bq2gcs.yaml\n",
    "\n",
    "name: bq2gcs\n",
    "    \n",
    "description: |\n",
    "    This component creates the training and\n",
    "    validation datasets as BiqQuery tables and export\n",
    "    them into a Google Cloud Storage bucket at\n",
    "    gs://<BUCKET>/taxifare/data.\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/PROJECT/taxifare-bq2gcs\n",
    "        args: [\"--bucket\", {inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainjob.yaml\n",
    "\n",
    "name: trainjob\n",
    "    \n",
    "description: |\n",
    "    This component trains a model to predict that taxi fare in NY.\n",
    "    It takes as argument a GCS bucket and expects its training and\n",
    "    eval data to be at gs://<BUCKET>/taxifare/data/ and will export\n",
    "    the trained model at  gs://<BUCKET>/taxifare/model/.\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/PROJECT/taxifare-trainjob\n",
    "        args: [{inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deploymodel.yaml\n",
    "\n",
    "name: deploymodel\n",
    "    \n",
    "description: |\n",
    "    This component deploys a trained taxifare model on GCP as taxifare:dnn.\n",
    "    It takes as argument a GCS bucket and expects the model to deploy \n",
    "    to be found at gs://<BUCKET>/taxifare/model/export/savedmodel/\n",
    "        \n",
    "inputs:\n",
    "    - {name: Input Bucket , type: String, description: 'GCS directory path.'}\n",
    "\n",
    "implementation:\n",
    "    container:\n",
    "        image: gcr.io/PROJECT/taxifare-deploymodel\n",
    "        args: [{inputValue: Input Bucket}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a kubeflow pipeline by decorating a regular fuction with the\n",
    "`@dsl.pipeline` decorator. Now the arguments of this decorated function will be\n",
    "the input parameters of the Kubeflow pipeline.\n",
    "\n",
    "Inside the function, we describe the pipeline by\n",
    "* loading the yaml component files we created above into a Kubeflow op\n",
    "* specifying the order into which the Kubeflow ops should be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4\n",
    "PIPELINE_TAR = 'taxifare.tar.gz'\n",
    "BQ2GCS_YAML = './bq2gcs.yaml'\n",
    "TRAINJOB_YAML = './trainjob.yaml'\n",
    "DEPLOYMODEL_YAML = './deploymodel.yaml'\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Taxifare',\n",
    "    description='Train a ml model to predict the taxi fare in NY')\n",
    "def pipeline(gcs_bucket_name='<bucket where data and model will be exported>'):\n",
    "\n",
    "\n",
    "    bq2gcs_op = comp.load_component_from_file(BQ2GCS_YAML)\n",
    "    bq2gcs = bq2gcs_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "    trainjob_op = comp.load_component_from_file(TRAINJOB_YAML)\n",
    "    trainjob = trainjob_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "    deploymodel_op = comp.load_component_from_file(DEPLOYMODEL_YAML)\n",
    "    deploymodel = deploymodel_op(\n",
    "        input_bucket=gcs_bucket_name,\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "\n",
    "    trainjob.after(bq2gcs)\n",
    "    deploymodel.after(trainjob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline function above is then used by the Kubeflow compiler to create a Kubeflow pipeline artifact that can be either uploaded to the Kubeflow cluster from the UI, or programatically, as we will do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline, PIPELINE_TAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $PIPELINE_TAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you untar and uzip this pipeline artifact, you'll see that the compiler has transformed the\n",
    "Python description of the pipeline into yaml description!\n",
    "\n",
    "Now let's feed Kubeflow with our pipeline and run it using our client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5\n",
    "run = client.run_pipeline(\n",
    "    experiment_id=exp.id, \n",
    "    job_name='taxifare', \n",
    "    pipeline_package_path='taxifare.tar.gz', \n",
    "    params={\n",
    "        'gcs-bucket-name': \"dherin-sandbox\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the link to monitor the run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the runs are nicely organized under the experiment in the UI, and new runs can be either manually launched or scheduled through the UI in a completely repeatable and traceable way!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
