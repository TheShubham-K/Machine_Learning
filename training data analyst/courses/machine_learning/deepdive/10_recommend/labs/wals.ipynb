
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Collaborative filtering on Google Analytics data\n",
    "\n",
    "This notebook demonstrates how to implement a WALS matrix refactorization approach to do collaborative filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"cloud-training-demos\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"cloud-training-demos-ml\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create raw dataset\n",
    "<p>\n",
    "For collaborative filtering, we don't need to know anything about either the users or the content. Essentially, all we need to know is userId, itemId, and rating that the particular user gave the particular item.\n",
    "<p>\n",
    "In this case, we are working with newspaper articles. The company doesn't ask their users to rate the articles. However, we can use the time-spent on the page as a proxy for rating.\n",
    "<p>\n",
    "Normally, we would also add a time filter to this (\"latest 7 days\"), but our dataset is itself limited to a few days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project = PROJECT)\n",
    "\n",
    "sql = \"\"\"\n",
    "#standardSQL\n",
    "WITH CTE_visitor_page_content AS (\n",
    "    SELECT\n",
    "        fullVisitorID,\n",
    "        (SELECT MAX(IF(index=10, value, NULL)) FROM UNNEST(hits.customDimensions)) AS latestContentId,  \n",
    "        (LEAD(hits.time, 1) OVER (PARTITION BY fullVisitorId ORDER BY hits.time ASC) - hits.time) AS session_duration \n",
    "    FROM\n",
    "        `cloud-training-demos.GA360_test.ga_sessions_sample`,   \n",
    "        UNNEST(hits) AS hits\n",
    "    WHERE \n",
    "        # only include hits on pages\n",
    "        hits.type = \"PAGE\"\n",
    "\n",
    "    GROUP BY   \n",
    "        fullVisitorId,\n",
    "        latestContentId,\n",
    "        hits.time )\n",
    "\n",
    "-- Aggregate web stats\n",
    "SELECT   \n",
    "    fullVisitorID as visitorId,\n",
    "    latestContentId as contentId,\n",
    "    SUM(session_duration) AS session_duration\n",
    "FROM\n",
    "    CTE_visitor_page_content\n",
    "WHERE\n",
    "    latestContentId IS NOT NULL \n",
    "GROUP BY\n",
    "    fullVisitorID, \n",
    "    latestContentId\n",
    "HAVING \n",
    "    session_duration > 0\n",
    "ORDER BY \n",
    "    latestContentId \n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(sql).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stats = df.describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"session_duration\"]].plot(kind=\"hist\", logy=True, bins=100, figsize=[8,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The rating is the session_duration scaled to be in the range 0-1.  This will help with training.\n",
    "median = stats.loc[\"50%\", \"session_duration\"]\n",
    "df[\"rating\"] = 0.3 * df[\"session_duration\"] / median\n",
    "df.loc[df[\"rating\"] > 1, \"rating\"] = 1\n",
    "df[[\"rating\"]].plot(kind=\"hist\", logy=True, bins=100, figsize=[8,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del df[\"session_duration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data\n",
    "mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf = \"data/collab_raw.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!head data/collab_raw.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create dataset for WALS\n",
    "<p>\n",
    "The raw dataset (above) won't work for WALS:\n",
    "<ol>\n",
    "<li> The userId and itemId have to be 0,1,2 ... so we need to create a mapping from visitorId (in the raw data) to userId and contentId (in the raw data) to itemId.\n",
    "<li> We will need to save the above mapping to a file because at prediction time, we'll need to know how to map the contentId in the table above to the itemId.\n",
    "<li> We'll need two files: a \"rows\" dataset where all the items for a particular user are listed; and a \"columns\" dataset where all the users for a particular item are listed.\n",
    "</ol>\n",
    "\n",
    "<p>\n",
    "\n",
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def create_mapping(values, filename):\n",
    "    with open(filename, 'w') as ofp:\n",
    "        value_to_id = {value:idx for idx, value in enumerate(values.unique())}\n",
    "        for value, idx in value_to_id.items():\n",
    "            ofp.write(\"{},{}\\n\".format(value, idx))\n",
    "    return value_to_id\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer = \"data/collab_raw.csv\",\n",
    "                 header = None,\n",
    "                 names = [\"visitorId\", \"contentId\", \"rating\"],\n",
    "                dtype = {\"visitorId\": str, \"contentId\": str, \"rating\": np.float})\n",
    "df.to_csv(path_or_buf = \"data/collab_raw.csv\", index = False, header = False)\n",
    "user_mapping = create_mapping(df[\"visitorId\"], \"data/users.csv\")\n",
    "item_mapping = create_mapping(df[\"contentId\"], \"data/items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!head -3 data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[\"userId\"] = df[\"visitorId\"].map(user_mapping.get)\n",
    "df[\"itemId\"] = df[\"contentId\"].map(item_mapping.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mapped_df = df[[\"userId\", \"itemId\", \"rating\"]]\n",
    "mapped_df.to_csv(path_or_buf = \"data/collab_mapped.csv\", index = False, header = False)\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating rows and columns datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "mapped_df = pd.read_csv(filepath_or_buffer = \"data/collab_mapped.csv\", header = None, names = [\"userId\", \"itemId\", \"rating\"])\n",
    "mapped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "NITEMS = np.max(mapped_df[\"itemId\"]) + 1\n",
    "NUSERS = np.max(mapped_df[\"userId\"]) + 1\n",
    "mapped_df[\"rating\"] = np.round(mapped_df[\"rating\"].values, 2)\n",
    "print(\"{} items, {} users, {} interactions\".format( NITEMS, NUSERS, len(mapped_df) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_by_items = mapped_df.groupby(\"itemId\")\n",
    "iter = 0\n",
    "for item, grouped in grouped_by_items:\n",
    "    print(item, grouped[\"userId\"].values, grouped[\"rating\"].values)\n",
    "    iter = iter + 1\n",
    "    if iter > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "grouped_by_items = mapped_df.groupby(\"itemId\")\n",
    "with tf.python_io.TFRecordWriter(\"data/users_for_item\") as ofp:\n",
    "    for item, grouped in grouped_by_items:\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [item])),\n",
    "            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"userId\"].values)),\n",
    "            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"rating\"].values))\n",
    "        }))\n",
    "        ofp.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_by_users = mapped_df.groupby(\"userId\")\n",
    "with tf.python_io.TFRecordWriter(\"data/items_for_user\") as ofp:\n",
    "    for user, grouped in grouped_by_users:\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [user])),\n",
    "            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"itemId\"].values)),\n",
    "            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"rating\"].values))\n",
    "        }))\n",
    "        ofp.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_by_users = mapped_df.groupby(\"userId\")\n",
    "N = 0\n",
    "with tf.python_io.TFRecordWriter(\"data/items_for_user_subset\") as ofp:\n",
    "    for user, grouped in grouped_by_users:\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "            \"key\": tf.train.Feature(int64_list = tf.train.Int64List(value = [user])),\n",
    "            \"indices\": tf.train.Feature(int64_list = tf.train.Int64List(value = grouped[\"itemId\"].values)),\n",
    "            \"values\": tf.train.Feature(float_list = tf.train.FloatList(value = grouped[\"rating\"].values))\n",
    "        }))\n",
    "        ofp.write(example.SerializeToString())    \n",
    "        N = N + 1\n",
    "        if N > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!ls -lrt data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To summarize, we created the following data files from collab_raw.csv:\n",
    "<ol>\n",
    "<li> ```collab_mapped.csv``` is essentially the same data as in ```collab_raw.csv``` except that ```visitorId``` and ```contentId``` which are business-specific have been mapped to ```userId``` and ```itemId``` which are enumerated in 0,1,2,....  The mappings themselves are stored in ```items.csv``` and ```users.csv``` so that they can be used during inference.\n",
    "<li> ```users_for_item``` contains all the users/ratings for each item in TFExample format\n",
    "<li> ```items_for_user``` contains all the items/ratings for each user in TFExample format\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train with WALS\n",
    "\n",
    "Once you have the dataset, do matrix factorization with WALS using the [WALSMatrixFactorization](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/factorization/WALSMatrixFactorization) in the contrib directory.\n",
    "This is an estimator model, so it should be relatively familiar.\n",
    "<p>\n",
    "As usual, we write an input_fn to provide the data to the model, and then create the Estimator to do train_and_evaluate.\n",
    "Because it is in contrib and hasn't moved over to tf.estimator yet, we use tf.contrib.learn.Experiment to handle the training loop.<p>\n",
    "Make sure to replace <strong># TODO</strong> in below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.contrib.factorization import WALSMatrixFactorization\n",
    "  \n",
    "def read_dataset(mode, args):\n",
    "    def decode_example(protos, vocab_size):\n",
    "        # TODO\n",
    "        return\n",
    "  \n",
    "  \n",
    "    def remap_keys(sparse_tensor):\n",
    "        # Current indices of our SparseTensor that we need to fix\n",
    "        bad_indices = sparse_tensor.indices # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n",
    "        # Current values of our SparseTensor that we need to fix\n",
    "        bad_values = sparse_tensor.values # shape = (current_batch_size * (number_of_items/users[i] + 1),)\n",
    "\n",
    "        # Since batch is ordered, the last value for a batch index is the user\n",
    "        # Find where the batch index chages to extract the user rows\n",
    "        # 1 where user, else 0\n",
    "        user_mask = tf.concat(values = [bad_indices[1:,0] - bad_indices[:-1,0], tf.constant(value = [1], dtype = tf.int64)], axis = 0) # shape = (current_batch_size * (number_of_items/users[i] + 1), 2)\n",
    "\n",
    "        # Mask out the user rows from the values\n",
    "        good_values = tf.boolean_mask(tensor = bad_values, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "        item_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 0)) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "        user_indices = tf.boolean_mask(tensor = bad_indices, mask = tf.equal(x = user_mask, y = 1))[:, 1] # shape = (current_batch_size,)\n",
    "\n",
    "        good_user_indices = tf.gather(params = user_indices, indices = item_indices[:,0]) # shape = (current_batch_size * number_of_items/users[i],)\n",
    "\n",
    "        # User and item indices are rank 1, need to make rank 1 to concat\n",
    "        good_user_indices_expanded = tf.expand_dims(input = good_user_indices, axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n",
    "        good_item_indices_expanded = tf.expand_dims(input = item_indices[:, 1], axis = -1) # shape = (current_batch_size * number_of_items/users[i], 1)\n",
    "        good_indices = tf.concat(values = [good_user_indices_expanded, good_item_indices_expanded], axis = 1) # shape = (current_batch_size * number_of_items/users[i], 2)\n",
    "\n",
    "        remapped_sparse_tensor = tf.SparseTensor(indices = good_indices, values = good_values, dense_shape = sparse_tensor.dense_shape)\n",
    "        return remapped_sparse_tensor\n",
    "\n",
    "    \n",
    "    def parse_tfrecords(filename, vocab_size):\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        files = tf.gfile.Glob(filename = os.path.join(args[\"input_path\"], filename))\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.map(map_func = lambda x: decode_example(x, vocab_size))\n",
    "        dataset = dataset.repeat(count = num_epochs)\n",
    "        dataset = dataset.batch(batch_size = args[\"batch_size\"])\n",
    "        dataset = dataset.map(map_func = lambda x: remap_keys(x))\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "  \n",
    "    def _input_fn():\n",
    "        features = {\n",
    "            WALSMatrixFactorization.INPUT_ROWS: parse_tfrecords(\"items_for_user\", args[\"nitems\"]),\n",
    "            WALSMatrixFactorization.INPUT_COLS: parse_tfrecords(\"users_for_item\", args[\"nusers\"]),\n",
    "            WALSMatrixFactorization.PROJECT_ROW: tf.constant(True)\n",
    "        }\n",
    "        return features, None\n",
    "\n",
    "    return _input_fn\n",
    "  \n",
    "    def input_cols():\n",
    "        return parse_tfrecords(\"users_for_item\", args[\"nusers\"])\n",
    "  \n",
    "    return _input_fn#_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This code is helpful in developing the input function. You don't need it in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def try_out():\n",
    "    with tf.Session() as sess:\n",
    "        fn = read_dataset(\n",
    "            mode = tf.estimator.ModeKeys.EVAL, \n",
    "            args = {\"input_path\": \"data\", \"batch_size\": 4, \"nitems\": NITEMS, \"nusers\": NUSERS})\n",
    "        feats, _ = fn()\n",
    "        \n",
    "        print(feats[\"input_rows\"].eval())\n",
    "        print(feats[\"input_rows\"].eval())\n",
    "\n",
    "try_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_top_k(user, item_factors, k):\n",
    "    all_items = tf.matmul(a = tf.expand_dims(input = user, axis = 0), b = tf.transpose(a = item_factors))\n",
    "    topk = tf.nn.top_k(input = all_items, k = k)\n",
    "    return tf.cast(x = topk.indices, dtype = tf.int64)\n",
    "    \n",
    "def batch_predict(args):\n",
    "    import numpy as np\n",
    "    with tf.Session() as sess:\n",
    "        estimator = tf.contrib.factorization.WALSMatrixFactorization(\n",
    "            num_rows = args[\"nusers\"], \n",
    "            num_cols = args[\"nitems\"],\n",
    "            embedding_dimension = args[\"n_embeds\"],\n",
    "            model_dir = args[\"output_dir\"])\n",
    "        \n",
    "        # This is how you would get the row factors for out-of-vocab user data\n",
    "        # row_factors = list(estimator.get_projections(input_fn=read_dataset(tf.estimator.ModeKeys.EVAL, args)))\n",
    "        # user_factors = tf.convert_to_tensor(np.array(row_factors))\n",
    "\n",
    "        # But for in-vocab data, the row factors are already in the checkpoint\n",
    "        user_factors = tf.convert_to_tensor(value = estimator.get_row_factors()[0]) # (nusers, nembeds)\n",
    "        # In either case, we have to assume catalog doesn\"t change, so col_factors are read in\n",
    "        item_factors = tf.convert_to_tensor(value = estimator.get_col_factors()[0])# (nitems, nembeds)\n",
    "\n",
    "        # For each user, find the top K items\n",
    "        topk = tf.squeeze(input = tf.map_fn(fn = lambda user: find_top_k(user, item_factors, args[\"topk\"]), elems = user_factors, dtype = tf.int64))\n",
    "        with file_io.FileIO(os.path.join(args[\"output_dir\"], \"batch_pred.txt\"), mode = \"w\") as f:\n",
    "            for best_items_for_user in topk.eval():\n",
    "                f.write(\",\".join(str(x) for x in best_items_for_user) + \"\\n\")\n",
    "\n",
    "# Online prediction returns row and column factors as needed\n",
    "def create_serving_input_fn(args):\n",
    "    def for_user_embeddings(userId):\n",
    "        # All items for this user (for user_embeddings)\n",
    "        items = tf.range(args[\"nitems\"], dtype = tf.int64)\n",
    "        users = userId * tf.ones(shape = [args[\"nitems\"]], dtype = tf.int64)\n",
    "        ratings = 0.1 * tf.ones_like(tensor = users, dtype = tf.float32)\n",
    "        return items, users, ratings, tf.constant(value = True, dtype = tf.bool)\n",
    "    \n",
    "    def for_item_embeddings(itemId):\n",
    "        # All users for this item (for item_embeddings)\n",
    "        users = tf.range(args[\"nusers\"], dtype = tf.int64)\n",
    "        items = itemId * tf.ones(shape = [args[\"nusers\"]], dtype = tf.int64)\n",
    "        ratings = 0.1 * tf.ones_like(tensor = users, dtype = tf.float32)\n",
    "        return items, users, ratings, tf.constant(value = False, dtype = tf.bool)\n",
    "    \n",
    "    def serving_input_fn():\n",
    "        feature_ph = {\n",
    "            \"userId\": tf.placeholder(dtype = tf.int64, shape = 1),\n",
    "            \"itemId\": tf.placeholder(dtype = tf.int64, shape = 1)\n",
    "        }\n",
    "\n",
    "        (items, users, ratings, project_row) = \\\n",
    "            tf.cond(pred = feature_ph[\"userId\"][0] < tf.constant(value = 0, dtype = tf.int64),\n",
    "                true_fn = lambda: for_item_embeddings(feature_ph[\"itemId\"]),\n",
    "                false_fn = lambda: for_user_embeddings(feature_ph[\"userId\"]))\n",
    "        rows = tf.stack(values = [users, items], axis = 1)\n",
    "        cols = tf.stack(values = [items, users], axis = 1)\n",
    "        input_rows = tf.SparseTensor(indices = rows, values = ratings, dense_shape = (args[\"nusers\"], args[\"nitems\"]))\n",
    "        input_cols = tf.SparseTensor(indices = cols, values = ratings, dense_shape = (args[\"nusers\"], args[\"nitems\"]))\n",
    "\n",
    "        features = {\n",
    "            WALSMatrixFactorization.INPUT_ROWS: input_rows,\n",
    "            WALSMatrixFactorization.INPUT_COLS: input_cols,\n",
    "            WALSMatrixFactorization.PROJECT_ROW: project_row\n",
    "        }\n",
    "        return tf.contrib.learn.InputFnOps(features = features, labels = None, default_inputs = feature_ph)\n",
    "    return serving_input_fn\n",
    "        \n",
    "def train_and_evaluate(args):\n",
    "    train_steps = int(0.5 + (1.0 * args[\"num_epochs\"] * args[\"nusers\"]) / args[\"batch_size\"])\n",
    "    steps_in_epoch = int(0.5 + args[\"nusers\"] / args[\"batch_size\"])\n",
    "    print(\"Will train for {} steps, evaluating once every {} steps\".format(train_steps, steps_in_epoch))\n",
    "    def experiment_fn(output_dir):\n",
    "        return tf.contrib.learn.Experiment(\n",
    "            tf.contrib.factorization.WALSMatrixFactorization(\n",
    "                num_rows = args[\"nusers\"], \n",
    "                num_cols = args[\"nitems\"],\n",
    "                embedding_dimension = args[\"n_embeds\"],\n",
    "                model_dir = args[\"output_dir\"]),\n",
    "            train_input_fn = read_dataset(tf.estimator.ModeKeys.TRAIN, args),\n",
    "            eval_input_fn = read_dataset(tf.estimator.ModeKeys.EVAL, args),\n",
    "            train_steps = train_steps,\n",
    "            eval_steps = 1,\n",
    "            min_eval_frequency = steps_in_epoch,\n",
    "            export_strategies = tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(serving_input_fn = create_serving_input_fn(args))\n",
    "        )\n",
    "\n",
    "    from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "    learn_runner.run(experiment_fn = experiment_fn, output_dir = args[\"output_dir\"])\n",
    "    \n",
    "    batch_predict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"wals_trained\", ignore_errors=True)\n",
    "train_and_evaluate({\n",
    "    \"output_dir\": \"wals_trained\",\n",
    "    \"input_path\": \"data/\",\n",
    "    \"num_epochs\": 0.05,\n",
    "    \"nitems\": NITEMS,\n",
    "    \"nusers\": NUSERS,\n",
    "\n",
    "    \"batch_size\": 512,\n",
    "    \"n_embeds\": 10,\n",
    "    \"topk\": 3\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!ls wals_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!head wals_trained/batch_pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Let's run it as Python module for just a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NITEMS\"] = str(NITEMS)\n",
    "os.environ[\"NUSERS\"] = str(NUSERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf wals.tar.gz wals_trained\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=walsmodel.task \\\n",
    "    --package-path=${PWD}/walsmodel \\\n",
    "    -- \\\n",
    "    --output_dir=${PWD}/wals_trained \\\n",
    "    --input_path=${PWD}/data \\\n",
    "    --num_epochs=0.01 --nitems=${NITEMS} --nusers=${NUSERS} \\\n",
    "    --job-dir=./tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get row and column factors\n",
    "\n",
    "Once you have a trained WALS model, you can get row and column factors (user and item embeddings) using the serving input function that we exported.  We'll look at how to use these in the section on building a recommendation system using deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile data/input.json\n",
    "{\"userId\": 4, \"itemId\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_DIR=$(ls wals_trained/export/Servo | tail -1)\n",
    "gcloud ml-engine local predict --model-dir=wals_trained/export/Servo/$MODEL_DIR --json-instances=data/input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%writefile data/input.json\n",
    "{\"userId\": -1, \"itemId\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_DIR=$(ls wals_trained/export/Servo | tail -1)\n",
    "gcloud ml-engine local predict --model-dir=wals_trained/export/Servo/$MODEL_DIR --json-instances=data/input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run on Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m cp data/* gs://${BUCKET}/wals/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/wals/model_trained\n",
    "JOBNAME=wals_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=walsmodel.task \\\n",
    "    --package-path=${PWD}/walsmodel \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --input_path=gs://${BUCKET}/wals/data \\\n",
    "    --num_epochs=10 --nitems=${NITEMS} --nusers=${NUSERS} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This took <b>10 minutes</b> for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<pre>\n",
    "# Copyright 2018 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
